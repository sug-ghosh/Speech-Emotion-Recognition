In [0]:
!pip install librosa soundfile numpy sklearn
!pip install SpeechRecognition
!apt-get install libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0
!apt-get install ffmpeg libav-tools
!pip install pyaudio
!pip install xgboost

In [0]:
import librosa
import soundfile
import os, glob, pickle
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from google.colab import drive
import speech_recognition as sr
import pandas as pd

In [0]:
from google.colab import drive
drive.mount('/content/drive')
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).

In [0]:
#Data Link : https://zenodo.org/record/1188976#.XrBhw_IzY5k
data_folder = "/content/drive/My Drive/speech-emotion-recognition-ravdess-data"
data_folder2 = "/content/drive/My Drive/speech-emotion-recognition-tess-data"
test_folder = "/content/drive/My Drive/speech-emotion-recognition-test"
folders = os.listdir(data_folder)

In [0]:
emotions = {
  '01':'neutral',
  '02':'calm',
  '03':'happy',
  '04':'sad',
  '05':'angry',
  '06':'fearful',
  '07':'disgust',
  '08':'surprised'
}
classes = {
  'calm':'calm',
  'happy':'happy',
  'sad':'sad',
  'angry':'angry',
  'fearful':'fearful',
  'surprised':'surprised'
}

unobserved_emotions = ['neutral', 'disgust']

In [0]:
def noise(data):
    noise_amp = 0.05*np.random.uniform()*np.amax(data)   # more noise reduce the value to 0.5
    data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0])
    return data

In [0]:
def extract_feature(file_name, mfcc, mel):
  with soundfile.SoundFile(file_name) as sound_file:
    X = sound_file.read(dtype = "float32")          # read sound as float
    X = noise(X)
    sample_rate = sound_file.samplerate
    result = np.array([])
    if mfcc:
        mfccs = np.mean(librosa.feature.mfcc(y = X, sr = sample_rate, n_mfcc = 30).T, axis = 0)
        result = np.hstack((result, mfccs))     # create matrix
    
    if mel:
        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)
        result=np.hstack((result, mel))
  return result
In [0]:
def load_data(test_size = 0.2):
  x,y = [],[]
  for file in glob.glob(data_folder+"/Actor_*/*.wav"):
      file_name = os.path.basename(file)
      emotion = emotions[file_name.split("-")[2]]
      if emotion in unobserved_emotions:
          continue
      feature = extract_feature(file, mfcc = True, mel= False)
      x.append(feature)
      y.append(emotion)

  for file in glob.glob(data_folder2+"/*/*/*.wav"):
    file_name = os.path.basename(file)
    emotion = file_name.split("_")[0]
    if emotion in unobserved_emotions:
        continue
    feature = extract_feature(file, mfcc = True, mel=False)
    x.append(feature)
    y.append(emotion)
  return train_test_split(np.array(x), y, test_size = test_size, random_state = 9)
  
In [0]:
def load_data_test():
  x,y = [],[]
  for file in glob.glob(test_folder+"/*.wav"):
      file_name = os.path.basename(file)
      emotion = file_name.split("_")[0]
      if emotion in unobserved_emotions:
          continue
      feature = extract_feature(file, mfcc = True, mel= False)
      x.append(feature)
      y.append(emotion)
  return np.array(x), y

In [0]:
x_train,x_test,y_train,y_test = load_data(test_size = 0.20)

In [0]:
print((x_train.shape[0], x_test.shape[0]))
print(f'Features extracted: {x_train.shape[1]}')

In [0]:
from sklearn.neural_network import MLPClassifier
MLP = MLPClassifier(alpha = 0.01, batch_size = 32, hidden_layer_sizes = (64,32,32,16,8), learning_rate = 'adaptive', max_iter = 215)

In [0]:
from xgboost import XGBClassifier
xgb = XGBClassifier(colsample_bytree=0.2, gamma=0.0468, 
                             learning_rate=0.1, max_depth=4, 
                             min_child_weight=1.7817, n_estimators=100,
                             reg_alpha=0.4640, reg_lambda=1.25,
                             subsample=0.2, silent=0,
                             random_state =7, nthread = -1)  
                             
In [0]:
MLP.fit(x_train, y_train)
train_pred = MLP.predict(x_test)
# evaluate predictions
accuracy = accuracy_score(y_test, train_pred)
print("accuracy: %.2f%%" % (accuracy))
#get results
x_axis = range(0, MLP.n_iter_)
fig, ax = plt.subplots()
ax.plot(x_axis, MLP.loss_curve_, label='Train')
ax.legend()
plt.ylabel('Classification Error')
plt.title('MLP Classification Error')
plt.show()

In [0]:
import matplotlib.pyplot as plt
eval_set = [(x_train, y_train), (x_test, y_test)]
xgb.fit(x_train, y_train, eval_metric=["merror"], eval_set=eval_set, verbose=True)
train_pred = xgb.predict(x_test)
# evaluate predictions
accuracy = accuracy_score(y_test, train_pred)
print("accuracy: %.2f%%" % (accuracy))
#get results
results = xgb.evals_result()
epochs = len(results['validation_0']['merror'])
x_axis = range(0, epochs)
fig, ax = plt.subplots()
ax.plot(x_axis, results['validation_0']['merror'], label='Train')
ax.plot(x_axis, results['validation_1']['merror'], label='Test')
ax.legend()
plt.ylabel('Classification Error')
plt.title('XGBoost Classification Error')
plt.show()

In [0]:
from sklearn.ensemble import  VotingClassifier
mlpxgb = VotingClassifier(estimators=[('xgb', xgb), ('mlp', MLP)], voting='hard')
mlpxgb = mlpxgb.fit(x_train, y_train)

In [0]:
mlp_pred = MLP.predict(x_test)
xgb_pred = xgb.predict(x_test)
mlpxgb_pred = mlpxgb.predict(x_test)

In [0]:
hx_test, hy_test = load_data_test()
mlphy_pred = MLP.predict(hx_test)
xgbhy_pred = xgb.predict(hx_test)
mlpxgbhy_pred = mlpxgb.predict(hx_test)

In [0]:
accuracy = accuracy_score(y_true = hy_test, y_pred = mlphy_pred)
#Print the accuracy
print("Accuracy for MLP: {:.2f}%".format(accuracy*100))

accuracy = accuracy_score(y_true = hy_test, y_pred = xgbhy_pred)
#Print the accuracy
print("Accuracy for xgBoost: {:.2f}%".format(accuracy*100))

accuracy = accuracy_score(y_true = hy_test, y_pred = mlpxgbhy_pred)
#Print the accuracy
print("Accuracy for ensemble: {:.2f}%".format(accuracy*100))

In [0]:
print("mlp: {}".format(mlphy_pred))
print("xgb: {}".format(xgbhy_pred))
print("mlpxgb: {}".format(mlpxgbhy_pred))
print("true: {}".format(hy_test))
train_xg = xgb.predict(x_train)
train_mlp = MLP.predict(x_train)
train_mlpxgb = mlpxgb.predict(x_train)

In [0]:
from sklearn.metrics import classification_report
# print(classification_report(y_train, train_xg, target_names=classes))
# print(classification_report(y_train, train_mlp, target_names=classes))
# print(classification_report(y_train, train_mlpxgb, target_names=classes))

print("xgb: {}".format(classification_report(y_test, xgb_pred, target_names=classes)))
print("mlp: {}".format(classification_report(y_test, mlp_pred, target_names=classes)))
print("ensemble: {}".format(classification_report(y_test, mlpxgb_pred, target_names=classes)))

In [0]:
accuracy = accuracy_score(y_true = y_test, y_pred = mlp_pred)
#Print the accuracy
print("Accuracy for mlp: {:.2f}%".format(accuracy*100))
accuracy = accuracy_score(y_true = y_test, y_pred = xgb_pred)
#Print the accuracy
print("Accuracy for xgBoost: {:.2f}%".format(accuracy*100))
accuracy = accuracy_score(y_true = y_test, y_pred = mlpxgb_pred)
#Print the accuracy
print("Accuracy for mlpxgb: {:.2f}%".format(accuracy*100))

In [0]:
#1D CNN
import keras
from keras import regularizers
from keras.models import Sequential, Model
from keras.layers import Dense, Embedding
from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization
from keras.layers import Conv1D, MaxPooling1D
from keras.utils import np_utils, to_categorical
from keras import losses, models, optimizers
from keras.activations import relu, softmax
# sklearn
from sklearn.preprocessing import LabelEncoder

In [0]:
# preparation steps to get it into the correct format for Keras 
X_train = np.array(x_train)
y_train = np.array(y_train)
X_test = np.array(x_test)
y_test = np.array(y_test)

# one hot encode the target 
lb = LabelEncoder()
y_train = np_utils.to_categorical(lb.fit_transform(y_train))
y_test = np_utils.to_categorical(lb.fit_transform(y_test))

print(X_train.shape)
print(lb.classes_)

In [0]:
X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)
X_train.shape
#Dimension preps for 1D CNN

In [0]:
model = Sequential()
model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns
model.add(Activation('relu'))
model.add(Conv1D(256, 8, padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))
model.add(MaxPooling1D(pool_size=(6)))
model.add(Conv1D(128, 8, padding='same'))
model.add(Activation('relu'))
model.add(Conv1D(128, 8, padding='same'))
model.add(Activation('relu'))
model.add(Conv1D(128, 8, padding='same'))
model.add(Activation('relu'))
model.add(Conv1D(128, 8, padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))
model.add(MaxPooling1D(pool_size=(5)))
model.add(Conv1D(64, 8, padding='same'))
model.add(Activation('relu'))
model.add(Conv1D(64, 8, padding='same'))
model.add(Activation('relu'))
model.add(Flatten())
model.add(Dense(6)) # Target class number
model.add(Activation('softmax'))
opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)
model.summary()

In [0]:
#train
model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])
model_history=model.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_test, y_test))

In [0]:
#predict
import matplotlib.pyplot as plt
#model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
score = model.evaluate(X_test, y_test, verbose=0)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

print(model_history.history.keys())

plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()



plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

In [0]:
preds = model.predict(X_test, 
                         batch_size=16, 
                         verbose=1)

preds=preds.argmax(axis=1)

In [0]:
# hy_test1 = np_utils.to_categorical(lb.fit_transform(hy_test))
hx_test1 = np.expand_dims(hx_test, axis=2)
hy_preds =model.predict(hx_test1)

hy_preds = hy_preds.argmax(axis=1)
In [0]:
hy_preds = hy_preds.astype(int).flatten()
hy_preds = (lb.inverse_transform((hy_preds)))
# hy_preds = (lb.inverse_transform((hy_preds)))
In [0]:
print(hy_preds)
print(hy_test)

In [0]:
preds1 = preds.astype(int).flatten()
preds1 = (lb.inverse_transform((preds1)))
#preds1 = (lb.inverse_transform((preds1)))
actual= y_test.argmax(axis=1)
actual = actual.astype(int).flatten()
actual = (lb.inverse_transform((actual)))
In [0]:
#print confusion matrix
print(classification_report(actual, preds1, target_names=classes))
